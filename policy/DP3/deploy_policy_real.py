import numpy as np
import torch
import hydra
import dill
import sys, os

current_file_path = os.path.abspath(__file__)
parent_directory = os.path.dirname(current_file_path)
sys.path.append(os.path.join(parent_directory, '3D-Diffusion-Policy'))

sys.path.append('/workspace/third_party/gello_software_bp')


from hydra import initialize, compose
from datetime import datetime
from omegaconf import OmegaConf
from dp3_policy import DP3


from dataclasses import dataclass
from typing import Optional, Tuple

import tyro
from gello.env import RobotEnv
from gello.robots.robot import PrintRobot
from gello.zmq_core.robot_node import ZMQClientRobot
from gello.zmq_core.camera_node import ZMQClientCamera
import torch
from collections import deque
from PIL import Image
import cv2
import time


def rgb_depth_to_pointcloud(
    rgb_bgr: np.ndarray,          # (H, W, 3), uint8, BGR order (as in your code)
    depth_raw: np.ndarray,          # (H, W), float32, in meters
    fx: float, fy: float,
    cx: float, cy: float,
    depth_scale: float = 1.0,
    k_points: int = 1024,
    min_depth: float = 0.0,
    max_depth: float = float('inf'),
    seed: int = 42
) -> np.ndarray:  # (k_points, 6) â†’ [x,y,z,r,g,b], rgb âˆˆ [0,1]
    """
    Convert aligned RGB (BGR uint8) and depth (meters) to downsampled colored point cloud.
    """
    depth_m = depth_raw * depth_scale
    # æ£€æŸ¥ depth_m æ˜¯å¦æœ‰3ä¸ªç»´åº¦ï¼Œå¹¶ä¸”æœ€åä¸€ä¸ªç»´åº¦æ˜¯1
    if depth_m.ndim == 3 and depth_m.shape[2] == 1:
        # å‹ç¼©æ‰æœ€åä¸€ä¸ªç»´åº¦ï¼Œä½¿å…¶ä» (H, W, 1) å˜ä¸º (H, W)
        depth_m = np.squeeze(depth_m, axis=2)

    H, W = depth_m.shape  # ç°åœ¨ depth_m.shape åº”è¯¥æ˜¯ (H, W)

    if rgb_bgr.shape[:2] != (H, W):
        # Resize depth to match RGB if needed (or vice versa)
        # In your case, you likely ensure they match, but we resize depth to RGB size
        depth_m = cv2.resize(depth_m, (rgb_bgr.shape[1], rgb_bgr.shape[0]), interpolation=cv2.INTER_NEAREST)

    # Step 1: Filter depth by valid range
    depth_valid = depth_m.copy()
    if np.isfinite(min_depth):
        depth_valid[depth_valid < min_depth] = 0.0
    if np.isfinite(max_depth):
        depth_valid[depth_valid > max_depth] = 0.0

    # Step 2: Project to 3D
    u = np.arange(W, dtype=np.float32)
    v = np.arange(H, dtype=np.float32)
    uu, vv = np.meshgrid(u, v)  # (H, W)

    z = depth_valid.astype(np.float32)
    valid = np.isfinite(z) & (z > 0.0)  # (H, W)

    x = (uu - cx) * z / fx
    y = (vv - cy) * z / fy
    xyz = np.stack([x, y, z], axis=-1).reshape(-1, 3)  # (H*W, 3)
    valid_flat = valid.reshape(-1)

    # Step 3: Get RGB (convert BGR â†’ RGB, then to [0,1])
    rgb_rgb = cv2.cvtColor(rgb_bgr, cv2.COLOR_BGR2RGB)  # (H, W, 3) uint8 â†’ RGB
    rgb_flat = rgb_rgb.reshape(-1, 3).astype(np.float32) / 255.0  # (H*W, 3), [0,1]

    # Step 4: Keep only valid points
    xyz_valid = xyz[valid_flat]
    rgb_valid = rgb_flat[valid_flat]

    # Step 5: Downsample to k_points
    rng = np.random.default_rng(seed)
    N = xyz_valid.shape[0]
    if N == 0:
        return np.zeros((k_points, 6), dtype=np.float32)
    if N >= k_points:
        idx = rng.choice(N, size=k_points, replace=False)
    else:
        extra = rng.choice(N, size=k_points - N, replace=True)
        idx = np.concatenate([np.arange(N), extra])
        rng.shuffle(idx)
    xyz_k = xyz_valid[idx]
    rgb_k = rgb_valid[idx]

    # Step 6: Concatenate â†’ (K, 6)
    pc = np.concatenate([xyz_k, rgb_k], axis=1).astype(np.float32)  # (K, 6)
    return pc

def encode_obs(observation):
    depth_scale = 0.0002500000118743628
    fx, fy, cx, cy = 601.6535034179688, 601.79345703125, 325.8245849609375, 237.5863494873047
    k_points=1024
    obs = dict()

    # print(observation["base_depth"].min(), observation["base_depth"].max())
    depth_mm = observation["base_depth"]
    rgb_bgr = observation["base_rgb"]  # (H, W, 3), uint8, BGR

    # ç”Ÿæˆç‚¹äº‘
    pointcloud = rgb_depth_to_pointcloud(
        rgb_bgr=rgb_bgr,
        depth_raw=depth_mm,
        fx=fx, fy=fy, cx=cx, cy=cy,
        depth_scale=depth_scale,
        k_points=1024,
        min_depth=0.1,   # å¯é€‰ï¼šè¿‡æ»¤å¤ªè¿‘çš„å™ªå£°
        max_depth=2.0,   # å¯é€‰ï¼šè¿‡æ»¤å¤ªè¿œçš„ç‚¹
        seed=42
    )  # shape: (1024, 6)

    # observation["base_rgb"] = observation["base_rgb"][:,:,[2,1,0]] # RGB to BGR
    obs['point_cloud'] = pointcloud
    
    position = observation["joint_positions"].astype(np.float32)
    if position[-1] > 0.5:
        position[-1] = 1.0  
    else:
        position[-1] = 0.0

    obs["agent_pos"] = position
    return obs

def get_model(usr_args): 
    # ckpt_file = f"{usr_args['ckpt_path']}/{usr_args['checkpoint_num']}.ckpt"
    config_path = "./3D-Diffusion-Policy/diffusion_policy_3d/config"
    config_name = f"{usr_args['config_name']}.yaml"
    with initialize(config_path=config_path, version_base='1.2'):
        cfg = compose(config_name=config_name)

    now = datetime.now()
    run_dir = f"data/outputs/{now:%Y.%m.%d}/{now:%H.%M.%S}_{usr_args['config_name']}_{usr_args['task_name']}"
    
    hydra_runtime_cfg = {
        "job": {
            "override_dirname": usr_args['task_name']
        },
        "run": {
            "dir": run_dir
        },
        "sweep": {
            "dir": run_dir,
            "subdir": "0"
        }
    }

    OmegaConf.set_struct(cfg, False)
    cfg.hydra = hydra_runtime_cfg
    cfg.task_name = usr_args["task_name"]
    cfg.expert_data_num = usr_args["expert_data_num"]
    cfg.raw_task_name = usr_args["task_name"]
    cfg.policy.use_pc_color = usr_args['use_rgb']
    OmegaConf.set_struct(cfg, True)

    return DP3(cfg, usr_args)

def reset_model(model):
    model.env_runner.reset_obs()

def resize_img(image, size=(320,240)):
    # print(image.shape)
    image = Image.fromarray(image)
    image = np.array(image.resize(size, Image.BILINEAR))
    # image = np.transpose(np.array(image), (1,2,0))
    # print(image.shape)
    return image 

@dataclass
class Args:
    agent: str = "none"
    robot_port: int = 6001
    wrist_camera_port: int = 5001
    base_camera_port: int = 5000
    hostname: str = "10.27.50.231" # ä¸»è¦ä¿®æ”¹è¿™ä¸ª
    robot_type: str = None  # only needed for quest agent or spacemouse agent
    hz: int = 100
    start_joints: Optional[Tuple[float, ...]] = None

    gello_port: Optional[str] = None
    mock: bool = False
    use_save_interface: bool = False
    data_dir: str = "/home/landau/gello_software/bc_data"
    task_name: str = 'default' 
    bimanual: bool = False
    verbose: bool = False

def main(args):
    import yaml
    yaml_file = 'deploy_policy.yml'  # å¯ä»¥æ˜¯ç›¸å¯¹è·¯å¾„æˆ–ç»å¯¹è·¯å¾„
    with open(yaml_file, 'r', encoding='utf-8') as file:
        usr_args = yaml.safe_load(file)  # ä½¿ç”¨ safe_load æ›´å®‰å…¨
    model = get_model(usr_args)

    if args.mock:
        robot_client = PrintRobot(8, dont_print=True)
        camera_clients = {}
    else:
        camera_clients = {
            # you can optionally add camera nodes here for imitation learning purposes
            # "wrist": ZMQClientCamera(port=args.wrist_camera_port, host=args.hostname),
            "base": ZMQClientCamera(port=args.base_camera_port, host=args.hostname),
        }
        robot_client = ZMQClientRobot(port=args.robot_port, host=args.hostname)
    env = RobotEnv(robot_client, control_rate_hz=args.hz, camera_dict=camera_clients)
    count = 0
    reset_model(model)

    # inference loop
    while True: 
        observation = env.get_obs()
        # show_image = cv2.cvtColor(observation['base_rgb'], cv2.COLOR_RGB2BGR)
        # success = cv2.imwrite(f'data/outputs/img/saved_image{count}.jpg', show_image)
        count+=1
        obs = encode_obs(observation)
        actions = model.get_action(obs)
        for act in actions:
            import copy
            ori_g = copy.deepcopy(act[-1])
            act[-1] = 0.0 if act[-1] > 0.5 else 1.0 # 0.12/0.7 are min./max. experimental gripper joint values
            print(f"{ori_g=}, gripper : {act[-1]}, {observation['joint_positions'][-1]}, {observation['gripper_position']}");
            # act[-1] = 0.0
            env.step(act)
            observation = env.get_obs()
            obs = encode_obs(observation)
            model.update_obs(obs)

# def main(args):
#     import h5py
#     import yaml
#     yaml_file = 'deploy_policy.yml'  # å¯ä»¥æ˜¯ç›¸å¯¹è·¯å¾„æˆ–ç»å¯¹è·¯å¾„
#     with open(yaml_file, 'r', encoding='utf-8') as file:
#         usr_args = yaml.safe_load(file)  # ä½¿ç”¨ safe_load æ›´å®‰å…¨
    
#     print("æ­£åœ¨åŠ è½½æ¨¡å‹...")
#     model = get_model(usr_args)

#     # --- [MODIFICATION START] ---
#     # ç§»é™¤æœºå™¨äºº/ZMQ/Envï¼Œä» HDF5 åŠ è½½
    
#     # 1. --- åœ¨æ­¤å¤„é…ç½®ä½ è¦æµ‹è¯•çš„æ–‡ä»¶å’Œå¸§ ---
#     HDF5_FILE_PATH = '/workspace/data_real/move_banana_to_box_dp3/demo_clean/data/0.hdf5' # æ›¿æ¢ä¸ºä½ çš„ HDF5 æ–‡ä»¶è·¯å¾„
#     FRAME_INDEX = 300  # æ›¿æ¢ä¸ºä½ æƒ³æµ‹è¯•çš„å¸§ç¼–å·
#     # ------------------------------------

#     print(f"æ­£åœ¨ä» {HDF5_FILE_PATH} åŠ è½½ç¬¬ {FRAME_INDEX} å¸§...")
    
#     # 2. --- ä» HDF5 åŠ è½½æ•°æ®ä»¥æ„å»º observation å­—å…¸ ---
#     observation = {}
#     try:
#         with h5py.File(HDF5_FILE_PATH, 'r') as f:
#             # åŠ è½½ RGB (JPEG å­—èŠ‚)
#             rgb_bytes = f['/observation/front_camera/rgb'][FRAME_INDEX]
#             # ä»å†…å­˜ç¼“å†²åŒºè§£ç 
#             rgb_np_array = np.frombuffer(rgb_bytes, dtype=np.uint8)
#             # cv2.imdecode é»˜è®¤è¯»å–ä¸º BGRï¼Œè¿™ç¬¦åˆ encode_obs çš„é¢„æœŸ
#             observation['base_rgb'] = cv2.imdecode(rgb_np_array, cv2.IMREAD_COLOR) 
            
#             # åŠ è½½ Depth
#             observation['base_depth'] = f['/observation/front_camera/depth_raw'][FRAME_INDEX]
            
#             # åŠ è½½ Velocities (è¿™æ˜¯ä½ çš„æ¨¡å‹ 'agent_pos' æœŸæœ›çš„è¾“å…¥, 7-dim)
#             observation['joint_positions'] = f['/joint_action/velocities'][FRAME_INDEX]

#     except Exception as e:
#         print(f"!! ä¸¥é‡é”™è¯¯: ä» HDF5 åŠ è½½æ•°æ®å¤±è´¥: {e}")
#         print("!! è¯·æ£€æŸ¥ HDF5_FILE_PATH, FRAME_INDEX, å’Œ HDF5 æ•°æ®é›†é”® (keys) æ˜¯å¦æ­£ç¡®ã€‚")
#         return

#     print("HDF5 æ•°æ®åŠ è½½æˆåŠŸ:")
#     print(f"  base_rgb shape: {observation['base_rgb'].shape}")
#     print(f"  base_depth shape: {observation['base_depth'].shape}")
#     print(f"  joint_positions: {observation['joint_positions']}")

#     # 3. --- åœ¨å•å¸§ä¸Šè¿è¡Œæ¨¡å‹ ---
    
#     # é‡ç½®æ¨¡å‹çŠ¶æ€ (ä¾‹å¦‚ï¼Œå¯¹äº diffusion, è¿™æ˜¯å¿…é¡»çš„)
#     reset_model(model) 

#     print("\næ­£åœ¨è°ƒç”¨ encode_obs(observation)...")
#     obs = encode_obs(observation)
#     print(f"  obs['agent_pos'] (å·²å¤„ç†): {obs['agent_pos']}")
    
#     print("æ­£åœ¨è°ƒç”¨ model.get_action(obs)...")
#     actions = model.get_action(obs)
    
#     print(f"\n--- ğŸš€ æ¨¡å‹é¢„æµ‹å®Œæˆ ---")
#     print(f"é¢„æµ‹çš„åŠ¨ä½œå— (Action Chunk) shape: {actions.shape}")

#     first_predicted_action = actions[0]
#     model_gripper_output = first_predicted_action[-1] # æ¨¡å‹è¾“å‡º (æ¥è¿‘ 0.0 æˆ– 1.0)
    
#     if model_gripper_output > 0.5: # æ¨¡å‹æƒ³è¦ "1.0" (å¼ å¼€)
#         final_gripper_cmd = 0.12
#         gripper_decision = f"(Open) (æ¨¡å‹åŸå§‹è¾“å‡º: {model_gripper_output:.4f} -> æ˜ å°„åˆ° {final_gripper_cmd})"
#     else: # æ¨¡å‹æƒ³è¦ "0.0" (é—­åˆ)
#         final_gripper_cmd = 0.7
#         gripper_decision = f"(Close) (æ¨¡å‹åŸå§‹è¾“å‡º: {model_gripper_output:.4f} -> æ˜ å°„åˆ° {final_gripper_cmd})"

#     print(f"\nç¬¬ä¸€ä¸ªé¢„æµ‹çš„åŠ¨ä½œ (Raw): {first_predicted_action}")
#     print(f"  - é¢„æµ‹çš„ 7-DoF é€Ÿåº¦: {first_predicted_action[:-1]}")
#     print(f"  - é¢„æµ‹çš„å¤¹çˆªæŒ‡ä»¤: {gripper_decision}")

if __name__ == '__main__':
    main(tyro.cli(Args))